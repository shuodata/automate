{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquin/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/joaquin/anaconda2/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'ShuoData: https://www.kaggle.com/shuodata'\n",
    "\n",
    "#just ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import visuals as vs\n",
    "import renders as rs\n",
    "from library import *\n",
    "from cleandf import *\n",
    "from someclf import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = Load_DataFrames('/Data/total.xlsx','DF_2')\n",
    "df_train = Load_DataFrames('/Data/total.xlsx','DF_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['people_id', 'activity_id', 'min_date', 'max_date', 'apg', 'ppg', 'date_t', 'wd_t', 'acts', 'group_1',\\\n",
    "        'wd', 'date','char_1_t', 'char_2_t', 'char_3_t', 'char_4_t', 'char_5_t', 'char_6_t', 'char_7_t',\\\n",
    "        'char_8_t', 'char_9_t', 'char_10_t', 'act', 'ac_1', 'ac_2', 'ac_3', 'ac_4', 'ac_5', 'ac_6', 'ac_7',  \\\n",
    "        'char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9', 'char_10', \\\n",
    "        'char_11', 'char_12', 'char_13', 'char_14', 'char_15', 'char_16', 'char_17', 'char_18', 'char_19', \\\n",
    "        'char_20', 'char_21', 'char_22', 'char_23', 'char_24', 'char_25', 'char_26', 'char_27', 'char_28', \\\n",
    "        'char_29', 'char_30', 'char_31', 'char_32', 'char_33', 'char_34', 'char_35', 'char_36', 'char_37', 'char_38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train[cols + ['outcome']]\n",
    "df_test = df_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train = df_train[df_train['group_1'] != 17304 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features and outcome from DataFrame...\n"
     ]
    }
   ],
   "source": [
    "print \"Extracting features and outcome from DataFrame...\"\n",
    "features = df_train.drop(['outcome','people_id','activity_id'], axis=1)\n",
    "outcome = df_train['outcome']\n",
    "\n",
    "df_train_droped = df_train.drop(['people_id','activity_id'], axis=1)\n",
    "\n",
    "df_test_droped = df_test.drop(['people_id','activity_id'], axis=1)\n",
    "\n",
    "## Produce a matrix for test data\n",
    "test_data = df_test.values.tolist()\n",
    "test_droped = df_test_droped.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 67)\n",
      "(498687, 66)\n"
     ]
    }
   ],
   "source": [
    "print(df_train_droped.shape)\n",
    "print(df_test_droped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = features, outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "def feature_red2(__X,__y):\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('reduce_dim', PCA()),\n",
    "        ('classify', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    N_FEATURES_OPTIONS = [8,16, 32]\n",
    "    criterion=['entropy']\n",
    "    max_depth=[63]\n",
    "    max_features=['auto']\n",
    "    min_samples_split=[2]\n",
    "    n_estimators=[18]\n",
    "    n_jobs=[2]\n",
    "    \n",
    "    param_grid = [\n",
    "        {\n",
    "            'reduce_dim': [PCA(iterated_power=7)],\n",
    "            'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "            'classify__criterion': criterion,\n",
    "            'classify__max_depth': max_depth,\n",
    "            'classify__max_features': max_features,\n",
    "            'classify__min_samples_split': min_samples_split,\n",
    "            'classify__n_jobs': n_jobs\n",
    "        },\n",
    "        {\n",
    "            'reduce_dim': [TruncatedSVD(n_iter=7, random_state=42)],\n",
    "            'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "            'classify__criterion': criterion,\n",
    "            'classify__max_depth': max_depth,\n",
    "            'classify__max_features': max_features,\n",
    "            'classify__min_samples_split': min_samples_split,\n",
    "            'classify__n_jobs': n_jobs\n",
    "        },\n",
    "        {\n",
    "            #'reduce_dim': [SelectKBest(chi2)],\n",
    "            'reduce_dim': [PCA(iterated_power=2)],\n",
    "            'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "            'classify__criterion': criterion,\n",
    "            'classify__max_depth': max_depth,\n",
    "            'classify__max_features': max_features,\n",
    "            'classify__min_samples_split': min_samples_split,\n",
    "            'classify__n_jobs': n_jobs\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    \n",
    "    grid = GridSearchCV(pipe, n_jobs=2, param_grid=param_grid)\n",
    "\n",
    "    grid.fit(__X, __y)\n",
    "\n",
    "    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr = feature_red2(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCA-RamdonForest.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(fr, 'PCA-RamdonForest.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classify': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "             max_depth=63, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=2, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       " 'classify__bootstrap': True,\n",
       " 'classify__class_weight': None,\n",
       " 'classify__criterion': 'entropy',\n",
       " 'classify__max_depth': 63,\n",
       " 'classify__max_features': 'auto',\n",
       " 'classify__max_leaf_nodes': None,\n",
       " 'classify__min_impurity_split': 1e-07,\n",
       " 'classify__min_samples_leaf': 1,\n",
       " 'classify__min_samples_split': 2,\n",
       " 'classify__min_weight_fraction_leaf': 0.0,\n",
       " 'classify__n_estimators': 10,\n",
       " 'classify__n_jobs': 2,\n",
       " 'classify__oob_score': False,\n",
       " 'classify__random_state': None,\n",
       " 'classify__verbose': 0,\n",
       " 'classify__warm_start': False,\n",
       " 'reduce_dim': PCA(copy=True, iterated_power=7, n_components=16, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'reduce_dim__copy': True,\n",
       " 'reduce_dim__iterated_power': 7,\n",
       " 'reduce_dim__n_components': 16,\n",
       " 'reduce_dim__random_state': None,\n",
       " 'reduce_dim__svd_solver': 'auto',\n",
       " 'reduce_dim__tol': 0.0,\n",
       " 'reduce_dim__whiten': False,\n",
       " 'steps': [('reduce_dim',\n",
       "   PCA(copy=True, iterated_power=7, n_components=16, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False)),\n",
       "  ('classify',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "               max_depth=63, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=10, n_jobs=2, oob_score=False, random_state=None,\n",
       "               verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = fr.predict(df_test_droped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission for PCA-RamdonForest at 20161111125850...\n"
     ]
    }
   ],
   "source": [
    "CreateSubmission(df_test, y_pred,\"PCA-RamdonForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
